{
  "metadata": {
    "dataset_name": "cais/mmlu",
    "dataset_types": {
      "test": "MMLU 평가용 테스트 데이터",
      "validation": "모델 검증용 데이터",
      "dev": "Few-shot 프롬프트용 샘플 데이터",
      "auxiliary_train": "모델 학습용 데이터"
    },
    "total_subjects": 57,
    "all_subjects": [
      "abstract_algebra",
      "anatomy",
      "astronomy",
      "business_ethics",
      "clinical_knowledge",
      "college_biology",
      "college_chemistry",
      "college_computer_science",
      "college_mathematics",
      "college_medicine",
      "college_physics",
      "computer_security",
      "conceptual_physics",
      "econometrics",
      "electrical_engineering",
      "elementary_mathematics",
      "formal_logic",
      "global_facts",
      "high_school_biology",
      "high_school_chemistry",
      "high_school_computer_science",
      "high_school_european_history",
      "high_school_geography",
      "high_school_government_and_politics",
      "high_school_macroeconomics",
      "high_school_mathematics",
      "high_school_microeconomics",
      "high_school_physics",
      "high_school_psychology",
      "high_school_statistics",
      "high_school_us_history",
      "high_school_world_history",
      "human_aging",
      "human_sexuality",
      "international_law",
      "jurisprudence",
      "logical_fallacies",
      "machine_learning",
      "management",
      "marketing",
      "medical_genetics",
      "miscellaneous",
      "moral_disputes",
      "moral_scenarios",
      "nutrition",
      "philosophy",
      "prehistory",
      "professional_accounting",
      "professional_law",
      "professional_medicine",
      "professional_psychology",
      "public_relations",
      "security_studies",
      "sociology",
      "us_foreign_policy",
      "virology",
      "world_religions"
    ]
  },
  "data": {
    "test": {
      "description": "MMLU 평가용 테스트 데이터",
      "total_samples": 14042,
      "columns": [
        "question",
        "subject",
        "choices",
        "answer"
      ],
      "unique_subjects": 57,
      "subjects": [
        "abstract_algebra",
        "anatomy",
        "astronomy",
        "business_ethics",
        "clinical_knowledge",
        "college_biology",
        "college_chemistry",
        "college_computer_science",
        "college_mathematics",
        "college_medicine",
        "college_physics",
        "computer_security",
        "conceptual_physics",
        "econometrics",
        "electrical_engineering",
        "elementary_mathematics",
        "formal_logic",
        "global_facts",
        "high_school_biology",
        "high_school_chemistry",
        "high_school_computer_science",
        "high_school_european_history",
        "high_school_geography",
        "high_school_government_and_politics",
        "high_school_macroeconomics",
        "high_school_mathematics",
        "high_school_microeconomics",
        "high_school_physics",
        "high_school_psychology",
        "high_school_statistics",
        "high_school_us_history",
        "high_school_world_history",
        "human_aging",
        "human_sexuality",
        "international_law",
        "jurisprudence",
        "logical_fallacies",
        "machine_learning",
        "management",
        "marketing",
        "medical_genetics",
        "miscellaneous",
        "moral_disputes",
        "moral_scenarios",
        "nutrition",
        "philosophy",
        "prehistory",
        "professional_accounting",
        "professional_law",
        "professional_medicine",
        "professional_psychology",
        "public_relations",
        "security_studies",
        "sociology",
        "us_foreign_policy",
        "virology",
        "world_religions"
      ],
      "subject_stats": {
        "abstract_algebra": 100,
        "anatomy": 135,
        "astronomy": 152,
        "business_ethics": 100,
        "clinical_knowledge": 265,
        "college_biology": 144,
        "college_chemistry": 100,
        "college_computer_science": 100,
        "college_mathematics": 100,
        "college_medicine": 173,
        "college_physics": 102,
        "computer_security": 100,
        "conceptual_physics": 235,
        "econometrics": 114,
        "electrical_engineering": 145,
        "elementary_mathematics": 378,
        "formal_logic": 126,
        "global_facts": 100,
        "high_school_biology": 310,
        "high_school_chemistry": 203,
        "high_school_computer_science": 100,
        "high_school_european_history": 165,
        "high_school_geography": 198,
        "high_school_government_and_politics": 193,
        "high_school_macroeconomics": 390,
        "high_school_mathematics": 270,
        "high_school_microeconomics": 238,
        "high_school_physics": 151,
        "high_school_psychology": 545,
        "high_school_statistics": 216,
        "high_school_us_history": 204,
        "high_school_world_history": 237,
        "human_aging": 223,
        "human_sexuality": 131,
        "international_law": 121,
        "jurisprudence": 108,
        "logical_fallacies": 163,
        "machine_learning": 112,
        "management": 103,
        "marketing": 234,
        "medical_genetics": 100,
        "miscellaneous": 783,
        "moral_disputes": 346,
        "moral_scenarios": 895,
        "nutrition": 306,
        "philosophy": 311,
        "prehistory": 324,
        "professional_accounting": 282,
        "professional_law": 1534,
        "professional_medicine": 272,
        "professional_psychology": 612,
        "public_relations": 110,
        "security_studies": 245,
        "sociology": 201,
        "us_foreign_policy": 100,
        "virology": 166,
        "world_religions": 171
      },
      "example": {
        "question": "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.",
        "subject": "abstract_algebra",
        "choices": [
          "0",
          "4",
          "2",
          "6"
        ],
        "answer": 1
      }
    },
    "validation": {
      "description": "모델 검증용 데이터",
      "total_samples": 1531,
      "columns": [
        "question",
        "subject",
        "choices",
        "answer"
      ],
      "unique_subjects": 57,
      "subjects": [
        "abstract_algebra",
        "anatomy",
        "astronomy",
        "business_ethics",
        "clinical_knowledge",
        "college_biology",
        "college_chemistry",
        "college_computer_science",
        "college_mathematics",
        "college_medicine",
        "college_physics",
        "computer_security",
        "conceptual_physics",
        "econometrics",
        "electrical_engineering",
        "elementary_mathematics",
        "formal_logic",
        "global_facts",
        "high_school_biology",
        "high_school_chemistry",
        "high_school_computer_science",
        "high_school_european_history",
        "high_school_geography",
        "high_school_government_and_politics",
        "high_school_macroeconomics",
        "high_school_mathematics",
        "high_school_microeconomics",
        "high_school_physics",
        "high_school_psychology",
        "high_school_statistics",
        "high_school_us_history",
        "high_school_world_history",
        "human_aging",
        "human_sexuality",
        "international_law",
        "jurisprudence",
        "logical_fallacies",
        "machine_learning",
        "management",
        "marketing",
        "medical_genetics",
        "miscellaneous",
        "moral_disputes",
        "moral_scenarios",
        "nutrition",
        "philosophy",
        "prehistory",
        "professional_accounting",
        "professional_law",
        "professional_medicine",
        "professional_psychology",
        "public_relations",
        "security_studies",
        "sociology",
        "us_foreign_policy",
        "virology",
        "world_religions"
      ],
      "subject_stats": {
        "abstract_algebra": 11,
        "anatomy": 14,
        "astronomy": 16,
        "business_ethics": 11,
        "clinical_knowledge": 29,
        "college_biology": 16,
        "college_chemistry": 8,
        "college_computer_science": 11,
        "college_mathematics": 11,
        "college_medicine": 22,
        "college_physics": 11,
        "computer_security": 11,
        "conceptual_physics": 26,
        "econometrics": 12,
        "electrical_engineering": 16,
        "elementary_mathematics": 41,
        "formal_logic": 14,
        "global_facts": 10,
        "high_school_biology": 32,
        "high_school_chemistry": 22,
        "high_school_computer_science": 9,
        "high_school_european_history": 18,
        "high_school_geography": 22,
        "high_school_government_and_politics": 21,
        "high_school_macroeconomics": 43,
        "high_school_mathematics": 29,
        "high_school_microeconomics": 26,
        "high_school_physics": 17,
        "high_school_psychology": 60,
        "high_school_statistics": 23,
        "high_school_us_history": 22,
        "high_school_world_history": 26,
        "human_aging": 23,
        "human_sexuality": 12,
        "international_law": 13,
        "jurisprudence": 11,
        "logical_fallacies": 18,
        "machine_learning": 11,
        "management": 11,
        "marketing": 25,
        "medical_genetics": 11,
        "miscellaneous": 86,
        "moral_disputes": 38,
        "moral_scenarios": 100,
        "nutrition": 33,
        "philosophy": 34,
        "prehistory": 35,
        "professional_accounting": 31,
        "professional_law": 170,
        "professional_medicine": 31,
        "professional_psychology": 69,
        "public_relations": 12,
        "security_studies": 27,
        "sociology": 22,
        "us_foreign_policy": 11,
        "virology": 18,
        "world_religions": 19
      },
      "example": {
        "question": "The cyclic subgroup of Z_24 generated by 18 has order",
        "subject": "abstract_algebra",
        "choices": [
          "4",
          "8",
          "12",
          "6"
        ],
        "answer": 0
      }
    },
    "dev": {
      "description": "Few-shot 프롬프트용 샘플 데이터",
      "total_samples": 285,
      "columns": [
        "question",
        "subject",
        "choices",
        "answer"
      ],
      "unique_subjects": 57,
      "subjects": [
        "abstract_algebra",
        "anatomy",
        "astronomy",
        "business_ethics",
        "clinical_knowledge",
        "college_biology",
        "college_chemistry",
        "college_computer_science",
        "college_mathematics",
        "college_medicine",
        "college_physics",
        "computer_security",
        "conceptual_physics",
        "econometrics",
        "electrical_engineering",
        "elementary_mathematics",
        "formal_logic",
        "global_facts",
        "high_school_biology",
        "high_school_chemistry",
        "high_school_computer_science",
        "high_school_european_history",
        "high_school_geography",
        "high_school_government_and_politics",
        "high_school_macroeconomics",
        "high_school_mathematics",
        "high_school_microeconomics",
        "high_school_physics",
        "high_school_psychology",
        "high_school_statistics",
        "high_school_us_history",
        "high_school_world_history",
        "human_aging",
        "human_sexuality",
        "international_law",
        "jurisprudence",
        "logical_fallacies",
        "machine_learning",
        "management",
        "marketing",
        "medical_genetics",
        "miscellaneous",
        "moral_disputes",
        "moral_scenarios",
        "nutrition",
        "philosophy",
        "prehistory",
        "professional_accounting",
        "professional_law",
        "professional_medicine",
        "professional_psychology",
        "public_relations",
        "security_studies",
        "sociology",
        "us_foreign_policy",
        "virology",
        "world_religions"
      ],
      "subject_stats": {
        "abstract_algebra": 5,
        "anatomy": 5,
        "astronomy": 5,
        "business_ethics": 5,
        "clinical_knowledge": 5,
        "college_biology": 5,
        "college_chemistry": 5,
        "college_computer_science": 5,
        "college_mathematics": 5,
        "college_medicine": 5,
        "college_physics": 5,
        "computer_security": 5,
        "conceptual_physics": 5,
        "econometrics": 5,
        "electrical_engineering": 5,
        "elementary_mathematics": 5,
        "formal_logic": 5,
        "global_facts": 5,
        "high_school_biology": 5,
        "high_school_chemistry": 5,
        "high_school_computer_science": 5,
        "high_school_european_history": 5,
        "high_school_geography": 5,
        "high_school_government_and_politics": 5,
        "high_school_macroeconomics": 5,
        "high_school_mathematics": 5,
        "high_school_microeconomics": 5,
        "high_school_physics": 5,
        "high_school_psychology": 5,
        "high_school_statistics": 5,
        "high_school_us_history": 5,
        "high_school_world_history": 5,
        "human_aging": 5,
        "human_sexuality": 5,
        "international_law": 5,
        "jurisprudence": 5,
        "logical_fallacies": 5,
        "machine_learning": 5,
        "management": 5,
        "marketing": 5,
        "medical_genetics": 5,
        "miscellaneous": 5,
        "moral_disputes": 5,
        "moral_scenarios": 5,
        "nutrition": 5,
        "philosophy": 5,
        "prehistory": 5,
        "professional_accounting": 5,
        "professional_law": 5,
        "professional_medicine": 5,
        "professional_psychology": 5,
        "public_relations": 5,
        "security_studies": 5,
        "sociology": 5,
        "us_foreign_policy": 5,
        "virology": 5,
        "world_religions": 5
      },
      "example": {
        "question": "Find all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.",
        "subject": "abstract_algebra",
        "choices": [
          "0",
          "1",
          "2",
          "3"
        ],
        "answer": 1
      }
    },
    "auxiliary_train": {
      "description": "모델 학습용 데이터",
      "total_samples": 99842,
      "columns": [
        "question",
        "subject",
        "choices",
        "answer"
      ],
      "unique_subjects": 0,
      "subjects": [],
      "subject_stats": {},
      "example": {
        "question": "Davis decided to kill Adams. He set out for Adams's house. Before he got there he saw Brooks, who resembled Adams. Thinking that Brooks was Adams, Davis shot at Brooks. The shot missed Brooks but wounded Case, who was some distance away. Davis had not seen Case. In a prosecution under a statute that proscribes any attempt to commit murder, the district attorney should indicate that the intended victim(s) was/were",
        "subject": "",
        "choices": [
          "Adams only.",
          "Brooks only.",
          "Case only.",
          "Adams and Brooks"
        ],
        "answer": 1
      }
    }
  }
}